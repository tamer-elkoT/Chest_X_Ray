{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc58ddf0",
   "metadata": {},
   "source": [
    "# Chest X-Ray Multi‑Class Project — Role Notebook\n",
    "\n",
    "**Dataset:** Kaggle “Lungs Disease Dataset (4 types)” by Omkar Manohar Dalvi  \n",
    "**Classes:** Normal, Bacterial Pneumonia, Viral Pneumonia, COVID‑19, Tuberculosis\n",
    "\n",
    "> Use this notebook in **Google Colab**. If you’re running locally, adapt the Drive mount steps accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2a685d",
   "metadata": {},
   "source": [
    "## Role — Member 2: Preprocessing & Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046e322",
   "metadata": {},
   "source": [
    "**Responsibilities**  \n",
    "- Implement resizing, normalization, augmentation  \n",
    "- Produce balanced input pipelines for train/val/test  \n",
    "- Export sample augmentations for QA  \n",
    "- Ensure deterministic splits & reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5c0430",
   "metadata": {},
   "source": [
    "## Environment & Paths\n",
    "\n",
    "- The code below mounts Google Drive (for persistence) and prepares base paths.  \n",
    "- Set `DATASET_DIR` to where the extracted dataset resides (after Kaggle download)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5838b6",
   "metadata": {},
   "source": [
    "## Augmentation Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c1b19",
   "metadata": {},
   "source": [
    "We use **tf.data** with `tf.image` for efficient pipelines. Augmentations include:\n",
    "- Random horizontal flip\n",
    "- Small rotations\n",
    "- Random brightness/contrast\n",
    "- Random zoom/crop\n",
    "\n",
    "> Keep medical realism: avoid extreme shears/rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Colab & Paths ===\n",
    "import os, sys, glob, json, random, shutil, time\n",
    "from pathlib import Path\n",
    "\n",
    "# If in Colab, mount Drive (safe to run elsewhere; it will just fail silently)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    IN_COLAB = True\n",
    "except Exception as e:\n",
    "    print(\"Not running on Colab or Drive not available:\", e)\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Project root inside Drive (you can change this)\n",
    "PROJECT_ROOT = Path('/content/drive/MyDrive/Chest_XRay_Project')\n",
    "PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Where the dataset will live (after download & unzip). Adjust as needed.\n",
    "DATASET_DIR = PROJECT_ROOT / 'lungs_dataset'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "REPORTS_DIR = PROJECT_ROOT / 'reports'\n",
    "\n",
    "for p in [OUTPUTS_DIR, MODELS_DIR, REPORTS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATASET_DIR :\", DATASET_DIR)\n",
    "print(\"OUTPUTS_DIR :\", OUTPUTS_DIR)\n",
    "print(\"MODELS_DIR  :\", MODELS_DIR)\n",
    "print(\"REPORTS_DIR :\", REPORTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0156845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TensorFlow setup ===\n",
    "import tensorflow as tf\n",
    "print(\"TF version:\", tf.__version__)\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "\n",
    "CLASS_NAMES = []\n",
    "for split in ['train']:\n",
    "    split_dir = DATASET_DIR / split\n",
    "    if split_dir.exists():\n",
    "        CLASS_NAMES = sorted([p.name for p in split_dir.iterdir() if p.is_dir()])\n",
    "        break\n",
    "print(\"Detected classes:\", CLASS_NAMES)\n",
    "\n",
    "# === tf.data loaders ===\n",
    "def make_ds(split, shuffle=True):\n",
    "    split_dir = str(DATASET_DIR / split)\n",
    "    ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        split_dir,\n",
    "        labels='inferred',\n",
    "        label_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=IMG_SIZE,\n",
    "        shuffle=shuffle,\n",
    "        seed=SEED\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "train_ds = make_ds('train', shuffle=True)\n",
    "val_ds   = make_ds('val',   shuffle=False)\n",
    "test_ds  = make_ds('test',  shuffle=False)\n",
    "\n",
    "# === Normalization layer ===\n",
    "normalizer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# === Augmentation — using Keras preprocessing layers ===\n",
    "data_augment = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.05),\n",
    "    tf.keras.layers.RandomZoom(0.05),\n",
    "    tf.keras.layers.RandomContrast(0.1),\n",
    "], name=\"augmentation\")\n",
    "\n",
    "def prep_pipeline(ds, training=False):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    ds = ds.map(lambda x, y: (normalizer(x), y), num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.map(lambda x, y: (data_augment(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.shuffle(1000, seed=SEED)\n",
    "    return ds.prefetch(AUTOTUNE)\n",
    "\n",
    "train_ds = prep_pipeline(train_ds, training=True)\n",
    "val_ds   = prep_pipeline(val_ds, training=False)\n",
    "test_ds  = prep_pipeline(test_ds, training=False)\n",
    "\n",
    "# Save class names for other notebooks\n",
    "with open(PROJECT_ROOT / 'classes.json', 'w') as f:\n",
    "    json.dump(CLASS_NAMES, f)\n",
    "print(\"Saved classes.json:\", PROJECT_ROOT / 'classes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb47a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Preview a few augmented samples ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_batch(ds, title=\"Batch preview\"):\n",
    "    images, labels = next(iter(ds))\n",
    "    plt.figure(figsize=(8,8))\n",
    "    for i in range(9):\n",
    "        plt.subplot(3,3,i+1)\n",
    "        plt.imshow(images[i].numpy())\n",
    "        idx = tf.argmax(labels[i]).numpy()\n",
    "        plt.title(CLASS_NAMES[idx])\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "show_batch(train_ds, title=\"Augmented training samples\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

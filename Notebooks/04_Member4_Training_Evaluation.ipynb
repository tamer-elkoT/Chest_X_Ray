{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05296979",
   "metadata": {},
   "source": [
    "# Chest X-Ray Multi‑Class Project — Role Notebook\n",
    "\n",
    "**Dataset:** Kaggle “Lungs Disease Dataset (4 types)” by Omkar Manohar Dalvi  \n",
    "**Classes:** Normal, Bacterial Pneumonia, Viral Pneumonia, COVID‑19, Tuberculosis\n",
    "\n",
    "> Use this notebook in **Google Colab**. If you’re running locally, adapt the Drive mount steps accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc40bc",
   "metadata": {},
   "source": [
    "## Role — Member 4: Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec303f57",
   "metadata": {},
   "source": [
    "**Responsibilities**  \n",
    "- Train models with callbacks (early stopping, LR schedule, checkpoints)  \n",
    "- Track metrics & plots  \n",
    "- Evaluate on test set; produce confusion matrix, classification report, ROC‑AUC (one‑vs‑rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e85fe9e",
   "metadata": {},
   "source": [
    "## Environment & Paths\n",
    "\n",
    "- The code below mounts Google Drive (for persistence) and prepares base paths.  \n",
    "- Set `DATASET_DIR` to where the extracted dataset resides (after Kaggle download)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f419ff",
   "metadata": {},
   "source": [
    "## Training Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5310f98c",
   "metadata": {},
   "source": [
    "1. Load the JSON architecture exported by Member 3  \n",
    "2. (Re)build with same weights config  \n",
    "3. Train with frozen backbone, then fine‑tune top layers  \n",
    "4. Save the best model and training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Colab & Paths ===\n",
    "import os, sys, glob, json, random, shutil, time\n",
    "from pathlib import Path\n",
    "\n",
    "# If in Colab, mount Drive (safe to run elsewhere; it will just fail silently)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    IN_COLAB = True\n",
    "except Exception as e:\n",
    "    print(\"Not running on Colab or Drive not available:\", e)\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Project root inside Drive (you can change this)\n",
    "PROJECT_ROOT = Path('/content/drive/MyDrive/Chest_XRay_Project')\n",
    "PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Where the dataset will live (after download & unzip). Adjust as needed.\n",
    "DATASET_DIR = PROJECT_ROOT / 'lungs_dataset'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "REPORTS_DIR = PROJECT_ROOT / 'reports'\n",
    "\n",
    "for p in [OUTPUTS_DIR, MODELS_DIR, REPORTS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATASET_DIR :\", DATASET_DIR)\n",
    "print(\"OUTPUTS_DIR :\", OUTPUTS_DIR)\n",
    "print(\"MODELS_DIR  :\", MODELS_DIR)\n",
    "print(\"REPORTS_DIR :\", REPORTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, json, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import itertools\n",
    "\n",
    "# Load classes\n",
    "with open(PROJECT_ROOT / 'classes.json') as f:\n",
    "    CLASS_NAMES = json.load(f)\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "IMG_SIZE = (224,224)\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "\n",
    "# Rebuild datasets (must match Member 2 preprocessing)\n",
    "train_raw = tf.keras.utils.image_dataset_from_directory(\n",
    "    str(DATASET_DIR / 'train'), label_mode='categorical',\n",
    "    image_size=IMG_SIZE, batch_size=BATCH_SIZE, seed=SEED)\n",
    "val_raw = tf.keras.utils.image_dataset_from_directory(\n",
    "    str(DATASET_DIR / 'val'), label_mode='categorical',\n",
    "    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_raw = tf.keras.utils.image_dataset_from_directory(\n",
    "    str(DATASET_DIR / 'test'), label_mode='categorical',\n",
    "    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "normalizer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "data_augment = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.05),\n",
    "    tf.keras.layers.RandomZoom(0.05),\n",
    "    tf.keras.layers.RandomContrast(0.1),\n",
    "])\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_raw.map(lambda x,y: (data_augment(normalizer(x), training=True), y)).prefetch(AUTOTUNE)\n",
    "val_ds   = val_raw.map(lambda x,y: (normalizer(x), y)).prefetch(AUTOTUNE)\n",
    "test_ds  = test_raw.map(lambda x,y: (normalizer(x), y)).prefetch(AUTOTUNE)\n",
    "\n",
    "# Load architecture and instantiate model (EfficientNetB0 backbone as default)\n",
    "from tensorflow.keras import models\n",
    "try:\n",
    "    with open(MODELS_DIR / 'model_for_training.json') as f:\n",
    "        model = models.model_from_json(f.read())\n",
    "except Exception as e:\n",
    "    print(\"Falling back to building a fresh EfficientNetB0:\", e)\n",
    "    base = tf.keras.applications.EfficientNetB0(include_top=False, input_shape=(224,224,3), weights='imagenet')\n",
    "    base.trainable = False\n",
    "    x = base.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    out = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=base.input, outputs=out, name='EffB0_TL')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.2, monitor='val_loss'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=str(MODELS_DIR / 'best_model.keras'),\n",
    "                                       save_best_only=True, monitor='val_loss')\n",
    "]\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=callbacks)\n",
    "\n",
    "# Plot training curves (matplotlib only, single figure)\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "y_true = []\n",
    "y_prob = []\n",
    "for x, y in test_ds:\n",
    "    y_true.append(y.numpy())\n",
    "    y_prob.append(model.predict(x, verbose=0))\n",
    "\n",
    "y_true = np.vstack(y_true)\n",
    "y_prob = np.vstack(y_prob)\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "y_true_idx = np.argmax(y_true, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_idx, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_idx, y_pred, target_names=CLASS_NAMES))\n",
    "\n",
    "# ROC-AUC (one-vs-rest)\n",
    "try:\n",
    "    auc_macro = roc_auc_score(y_true, y_prob, multi_class='ovr', average='macro')\n",
    "    print(\"Macro ROC-AUC:\", auc_macro)\n",
    "except Exception as e:\n",
    "    print(\"ROC-AUC computation issue:\", e)\n",
    "\n",
    "# Save artifacts\n",
    "np.save(OUTPUTS_DIR / 'cm.npy', cm)\n",
    "with open(OUTPUTS_DIR / 'classification_report.txt', 'w') as f:\n",
    "    f.write(classification_report(y_true_idx, y_pred, target_names=CLASS_NAMES))\n",
    "with open(OUTPUTS_DIR / 'metrics.json', 'w') as f:\n",
    "    json.dump({\"roc_auc_macro\": float(auc_macro) if 'auc_macro' in locals() else None}, f, indent=2)\n",
    "\n",
    "print(\"Saved confusion matrix and metrics to\", OUTPUTS_DIR)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
